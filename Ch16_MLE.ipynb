{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "This notebook illustrates maximum likelihood estimation and how to calculate different standard errors (from the information matrix, the gradients and the \"sandwich\" approach).\n",
    "\n",
    "The application is (on purpose) very simple: estimating the mean and variance of a random variable. Some of the subsequent notebooks more complicated models, for instance, GARCH models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages and Extra Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "printyellow (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Printf, LinearAlgebra, DelimitedFiles, Statistics, Optim, ForwardDiff\n",
    "\n",
    "include(\"jlFiles/printmat.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx  = readdlm(\"Data/FFdSizePs.csv\",',',skipstart=1)\n",
    "x   = xx[:,2]                 #returns for the smallest size portfolio\n",
    "xx  = nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Estimates\n",
    "\n",
    "of the mean $\\mu$ and the variance $\\sigma^2$.\n",
    "\n",
    "To compare with the MLE, we use $1/T$ in the variance estimation, not $1/(T-1)$, by using the `corrected=false` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTraditional estimates and their std:\u001b[22m\u001b[39m\n",
      "\n",
      "    estimate       std\n",
      "μ      0.042     0.010\n",
      "σ²     0.840     0.013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T = length(x)\n",
    "\n",
    "(μ_trad,σ²_trad) = (mean(x),var(x,corrected=false))\n",
    "\n",
    "std_trad = sqrt.([σ²_trad,2*σ²_trad^2]/T)   #standard errors, textbook formulas\n",
    "\n",
    "printblue(\"Traditional estimates and their std:\\n\")\n",
    "xx = [[μ_trad,σ²_trad] std_trad]\n",
    "printmat(xx,colNames=[\"estimate\",\"std\"],rowNames=[\"μ\",\"σ²\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Estimates from ML\n",
    "\n",
    "The next few cells define a log likelihood function and estimate the coefficients by maximizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The (log) Likelihood Function for Estimating the Parameters of a N(,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NormalLL (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function NormalLL(par,x)      #par are the parameters, x is the data\n",
    "    (μ,σ²) = par\n",
    "    LLt    = -(1/2)*log(2*pi) - (1/2)*log(σ²) .- (1/2)*(x.-μ).^2/σ²  #vector, all x[t]\n",
    "    LL     = sum(LLt)\n",
    "    return LL, LLt\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the Likelihood Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood value at par0: -11155.385\n"
     ]
    }
   ],
   "source": [
    "par0 = [0.0,1.0]                #initial parameter guess\n",
    "\n",
    "(LL,LLt) = NormalLL(par0,x)     #just trying the log likelihood fn\n",
    "\n",
    "printlnPs(\"log likelihood value at par0: \",LL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the Likelihood Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood at point estimate: -11088.409\n",
      "\n",
      "\u001b[34m\u001b[1mParameter estimates:\u001b[22m\u001b[39m\n",
      "\n",
      "    traditional          MLE\n",
      "μ         0.042        0.042\n",
      "σ²        0.840        0.840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sol = optimize(par->-NormalLL(par,x)[1],par0)  #minimize -LL\n",
    "\n",
    "parHat = Optim.minimizer(Sol)                 #the optimal solution \n",
    "\n",
    "printlnPs(\"log-likelihood at point estimate: \",-Optim.minimum(Sol))\n",
    "\n",
    "printblue(\"\\nParameter estimates:\\n\")\n",
    "xx = [[μ_trad,σ²_trad] parHat]\n",
    "printmat(xx,colNames=[\"traditional\",\"MLE\"],rowNames=[\"μ\",\"σ²\"],width=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Errors I: Information Matrix \n",
    "\n",
    "If the likelihood function is correctly specified, then MLE is typically asymptotically normally distributed as\n",
    "\n",
    "$\n",
    "\\sqrt{T}(\\hat{\\theta}-\\theta)  \\rightarrow^{d}N(0,V) \\: \\text{, where } \\: V=I(\\theta)^{-1}\\text{ with }\n",
    "$\n",
    "\n",
    "$\n",
    "I(\\theta) =-\\text{E}\\frac{\\partial^{2}\\ln L_t}{\\partial\\theta\\partial\\theta^{\\prime}}\n",
    "$\n",
    "\n",
    "where $I(\\theta)$ is the information matrix and $\\ln L_t$  is the contribution of period $t$ to the likelihood function.\n",
    "\n",
    "The code below calculates numerical derivatives. I does so by noticing that $\n",
    "\\text{E}\\frac{\\partial^{2}\\ln L_t}{\\partial\\theta\\partial\\theta^{\\prime}} = \n",
    "\\frac{\\partial^{2}\\text{E}\\ln L_t}{\\partial\\theta\\partial\\theta^{\\prime}} \n",
    "$,\n",
    "so we can differentiate the mean (across data points) log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mstandard errors:\u001b[22m\u001b[39m\n",
      "\n",
      "         traditional     MLE (InfoMat)\n",
      "μ              0.010             0.010\n",
      "σ²             0.013             0.013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ia = -ForwardDiff.hessian(par->mean(NormalLL(par,x)[2]),parHat)  #derivative of mean(LLt)\n",
    "\n",
    "Ia       = (Ia+Ia')/2         #to guarantee symmetry, fixes rounding errors\n",
    "vcv      = inv(Ia)/T\n",
    "std_hess = sqrt.(diag(vcv))\n",
    "\n",
    "printblue(\"standard errors:\\n\")\n",
    "xx = [std_trad std_hess]\n",
    "printmat(xx,colNames=[\"traditional\",\"MLE (InfoMat)\"],rowNames=[\"μ\",\"σ²\"],width=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Errors II: Gradients and Sandwich\n",
    "\n",
    "Alternatively, we can use the outer product of the gradients to calculate the\n",
    "information matrix as\n",
    "\n",
    "$\n",
    "J(\\theta)=\\text{E}\\left[  \\frac{\\partial\\ln L_t}{\\partial\\theta\n",
    "}\\frac{\\partial\\ln L_t}{\\partial\\theta^{\\prime}}\\right]\n",
    "$\n",
    "\n",
    "The code below fills row $t$ of a $T\\times 2$ matrix (called `δL`) with \n",
    "$\n",
    "\\frac{\\partial\\ln L_t}{\\partial\\theta}.\n",
    "$\n",
    "For each $t$, the outer product is a $2\\times2$ matrix, and then we average (each element) across $t$. This is done by calculating \n",
    "`J = δL'δL/T`,\n",
    "which is the same as \n",
    "```\n",
    "J = zeros(2,2)\n",
    "for t = 1:T\n",
    "    J = J + δL[t,:]*δL[t,:]'/T\n",
    "end\n",
    "```\n",
    "\n",
    "We could also use the \"sandwich\" estimator\n",
    "\n",
    "$\n",
    "V=I(\\theta)^{-1}J(\\theta)I(\\theta)^{-1}.\n",
    "$\n",
    "\n",
    "When data is *not* iid $N($), then the three variance-covariance matrices may differ, and the sandwich approach is often the most robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Std from Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mstandard errors:\u001b[22m\u001b[39m\n",
      "\n",
      "         traditional     MLE (InfoMat)   MLE (gradients)\n",
      "μ              0.010             0.010             0.010\n",
      "σ²             0.013             0.013             0.005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "δL = ForwardDiff.jacobian(par->NormalLL(par,x)[2],parHat)   #Tx2\n",
    "J         = δL'δL/T               #2xT * Tx2\n",
    "\n",
    "vcv       = inv(J)/T\n",
    "std_grad  = sqrt.(diag(vcv))                          #std from gradients\n",
    "\n",
    "printblue(\"standard errors:\\n\")\n",
    "xx = [std_trad std_hess std_grad]\n",
    "printmat(xx,colNames=[\"traditional\",\"MLE (InfoMat)\",\"MLE (gradients)\"],rowNames=[\"μ\",\"σ²\"],width=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Std from Sandwich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mstandard errors:\u001b[22m\u001b[39m\n",
      "\n",
      "         traditional     MLE (InfoMat)   MLE (gradients)    MLE (sandwich)\n",
      "μ              0.010             0.010             0.010             0.010\n",
      "σ²             0.013             0.013             0.005             0.036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vcv       = inv(Ia) * J * inv(Ia)/T\n",
    "std_sandw = sqrt.(diag(vcv))                          #std from sandwich\n",
    "\n",
    "printblue(\"standard errors:\\n\")\n",
    "xx = [std_trad std_hess std_grad std_sandw]\n",
    "printmat(xx,colNames=[\"traditional\",\"MLE (InfoMat)\",\"MLE (gradients)\",\"MLE (sandwich)\"],rowNames=[\"μ\",\"σ²\"],width=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this: replace the data series `x` with simulated data from a $N()$ distribution. Then, do the different standard errors get closer to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
